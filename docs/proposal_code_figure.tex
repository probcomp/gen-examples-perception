\begin{figure}[t]
\begin{minipage}[t]{0.6\textwidth}
\begin{lstlisting}
using GenLiteTF
using TensorFlow
tf = TensorFlow

num_input = width * height
num_output = 11

function conv2d(x, W)
  tf.nn.conv2d(x, W, [1, 1, 1, 1], "SAME")
end

function max_pool_2x2(x)
  tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], "SAME")
end

function initial_weight(shape)
  randn(Float32, shape...) * 0.001f0
end

function initial_bias(shape)
  fill(0.1f0, shape...)
end

network = @tf_function begin

  # input image (N, 56 * 56)
  @input image_flat Float32 [-1, num_input]
  image = tf.reshape(image_flat, [-1, width, height, 1])

  # convolution + max-pooling (N, 28, 28, 32)
  @param W_conv1 initial_weight([5, 5, 1, 32])
  @param b_conv1 initial_bias([32])
  h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)
  h_pool1 = max_pool_2x2(h_conv1)

  # convolution + max-pooling (N, 14, 14, 32)
  @param W_conv2 initial_weight([5, 5, 32, 32])
  @param b_conv2 initial_bias([32])
  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
  h_pool2 = max_pool_2x2(h_conv2)
  h_pool2_flat = tf.reshape(h_pool2, [-1, 14 * 14 * 32])

  # convolution + max-pooling (N, 7, 7, 64)
  @param W_conv3 initial_weight([5, 5, 32, 64])
  @param b_conv3 initial_bias([64])
  h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)
  h_pool3 = max_pool_2x2(h_conv3)
  h_pool3_flat = tf.reshape(h_pool3, [-1, 7 * 7 * 64])

  # fully connected layer (N, 1024)
  @param W_fc1 initial_weight([7 * 7 * 64, 1024])
  @param b_fc1 initial_bias([1024])
  h_fc1 = tf.nn.relu(h_pool3_flat * W_fc1 + b_fc1)

  # output layer (N, 11)
  @param W_fc2 initial_weight([1024, num_output])
  @param b_fc2 initial_bias([num_output])
  @output Float32 (tf.matmul(h_fc1, W_fc2) + b_fc2)
end

\end{lstlisting}
\end{minipage}%
\begin{minipage}[t]{0.4\textwidth}
\begin{lstlisting}
predict = @gen function (outputs)

  # predict the x-coordinate
  x_mu = outputs[1]
  x_std = exp(outputs[2])
  @rand(normal(x_mu, x_std), <@\addr{"x"}@>)

  # predict the y-coordinate
  y_mu = outputs[3]
  y_std = exp(outputs[4])
  @rand(normal(y_mu, y_std), <@\addr{"y"}@>)

  # predict the rotation
  r_mu = exp(outputs[5])
  r_std = exp(outputs[6])
  @rand(normal(r_mu, r_std), <@\addr{"angle"}@>)

  # predict the size 
  size_alpha = exp(outputs[7])
  size_beta = exp(outputs[8])
  @rand(Gen.beta(size_alpha, size_beta), <@\addr{"size"}@>)
  
  # predict the identity of the letter
  log_letter_dist = outputs[9:end]
  letter_dist = exp.(log_letter_dist)
  letter_dist = letter_dist / sum(letter_dist)
  @rand(categorical(letter_dist), <@\addr{"letter"}@>)
end

proposal = @gen function ()

  # get image from input trace
  image = zeros(1, num_input)
  image[1,:] = @read(<@\addr{"image"}@>)[:]

  # run inference network
  outputs = @tf_call(network(image))

  # make prediction given inference network outputs
  @call(predict(outputs[1,:]), <@\addr{"prediction"}@>)
end

proposal_batch = @gen function (batch_size)

  # get images from input trace
  images = zeros(Float32, batch_size, num_input)
  for i=1:batch_size
    images[i,:] = @read((<@\addr{"\$i"}@>, <@\addr{"image"}@>))[:]
  end

  # run inference network in batch
  outputs = @tf_call(network(images))
  
  # make prediction for each image,
  # given inference network outputs
  for i=1:batch_size
    @call(predict(outputs[i,:]), <@\addr{"\$i"}@>)
  end
end
\end{lstlisting}
\end{minipage}
\caption{}
\label{fig:proposal-code-figure}
\end{figure}


%x = width * @rand(uniform_cont(0, 1), <@\addr{"x"}@>)
