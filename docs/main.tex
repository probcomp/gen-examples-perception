\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage{nips_2017}
\usepackage{listings}
\usepackage{textcomp}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\usepackage{graphicx}
\usepackage[subrefformat=parens]{subcaption}

\newcommand{\expr}[0]{\texttt{<expr>}}
\newcommand{\address}[0]{\texttt{<address>}}

\input{genlisting}

\title{Customizable deep learning and Monte Carlo for probabilistic inference using probabilistic programs}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Contributions:
\begin{enumerate}
\item A universal probabilistic programming system that combines programmable Monte Carlo inference with scalable and customizable deep learning.
\item We show how to express data-driven proposals as probabilistic programs that embed custom deep neural networks, and how to train these proposals on data simulated from a generative model.
\item We show how a simple combination of model-based Monte Carlo and deep learning can improve upon pure deep learning based inference.
\end{enumerate}

\section{Background: Programmable Inference in GenLite}
GenLite \cite{TODO} is a flexible probabilistic programming language designed to support user-programmable inference.
This section provides a very brief introduction to GenLite.
GenLite is embedded in Julia \cite{TODO}.
In GenLite, users first express a probabilistic generative model as a \emph{generative function}, which is written in an embedded DSL that extends Julia functions with the ability to make traced random choies.
Then, users implement Monte Carlo inference algorithms for the model in Julia code, drawing heavily on GenLite's inference programming API, which provides core data structures and inference primitives that utilize the programmatic representation of the generative model.
User inference code is written at a higher level of abstraction than typical custom sampler implementations, which makes it more efficient to develop, maintain, and reason about.

In GenLite, a \emph{trace} is a Julia value (of type \texttt{Trace}) that contains a record of random choices made by some generative function.
Generative functions, which are declared with keyword \texttt{@gen function}, extend Julia with three language constructs:
\begin{enumerate}
\item \texttt{@rand(<expr>, <address>)}: Sample a random value (i.e. a `random choice') from probability distribution \expr{}, where \address{} is a string or tuple of strings that specify a hierarchical name, or `address' for the random choice.
Record the random choice in the trace at the given address.
\item \texttt{@call(<expr>, <address>)}: Invoke a generative function given by \expr{}, and record the resulting trace for the invoked function under address \address{} in the current trace.
\item \texttt{@read(<address>)}: If an input trace is provided, read the value in the input trace at the given \address{}.
\end{enumerate}

Note that generative functions can contain arbitrary Julia code.
Figure~\ref{fig:model-code-figure} shows a generative function that only exercises the \texttt{@rand} keyword.

\subsection{Using generative functions to define proposal distributions}
Like probabilistic models, we represent proposals as probabilistic programs.
Proposal programs can be used in importance sampling, sequential Monte Carlo, and Markov chain Monte Carlo.
Proposal programs can include latent variables.

\begin{lstlisting}[basicstyle=\ttfamily\small]
(new_trace::Trace, new_score::Float64, alpha::Float64, val) = mh4(
        model::GenerativeFunction, model_args::Tuple,
        forward::GenerativeFunction, forward_args::Tuple,
        backward::GenerativeFunction, backward_args::Tuple,
        transform::TraceTransform, transform_args::Tuple,
        prev_trace::Trace, prev_score::Float64)
\end{lstlisting}


\subsection{Training proposal distributions on simulated data}
Proposal programs can be used on simulated data (cite the `probabilitsic programs as proposals' research).
Proposal programs can be trained for use as importance distributions or MCMC proposals.
Show the math for training (KL divergence and maximum likelihood).
Discuss static parameters.

\section{Scalable Deep Learning in GenLite using TensorFlow}
We extend GenLite with a \texttt{tf\_function} and \texttt{tf\_call} language constructs.
The \texttt{tf\_function} keyword is used to define \emph{GenLite TensorFlow functions}, which are functional TensorFlow computations with declared inputs, trainable parameters, and an output.
Discuss the reverse-mode AD integration.
Discuss vectorized (batched) training.

\begin{figure}[h]
\centering
    \includegraphics[width=0.7\textwidth]{images/tf-integration-schematic.pdf}
    \caption{
Reverse-mode AD in GenLite interoperates with `GenLite TensorFlow functions', which are blocks of functional TensorFlow (TF) code with inputs (corresponding to TF placeholders), trainable parameters (corresponding to TF variables), and an output, that are invoked by GenLite generative functions.
Each invocation of a TF function produces a single element on GenLite's reverse-mode AD tape.
During the backward pass (solid lines), we receive the gradient with respect to the output (\texttt{@output}) of the TF function; TF is used to compute the gradients with respect to inputs (\texttt{@input}) and parameters (\texttt{@params}).
The gradients with respect to the parameters are accumulated across multiple backward passes, until an parameter update is performed.
A parameter update (dashed line) changes the parameter values using the accumulated gradient (in addition to state of the update operation itself), and resets the gradient accumulators to zero.
Parameter updates are TF computations that are defined by the user separately from the TF function itself.
}
    \label{fig:tf-integration-schematic}
\end{figure}

\section{Combining Monte Carlo Inference and Deep Learning Proposals}
Because GenLite provides programmable inference, we can combine deep neural network proposals with other Monte Carlo strategies, like random walk moves, for refining a hypothesis.

\include{model_code_figure}
\include{proposal_code_figure}
\include{training_code_figure}

\section{Example}

\begin{figure}[h]
\centering
    \includegraphics[width=1.0\textwidth]{images/deep-neural-network-is.pdf}
    \caption{
Inference in the generative model of Figure~\ref{fig:model-code-figure} using a combination of deep learning and model-based Monte Carlo.
On the left is the observed image, followed by a set of 10 of latent images sampled from the trained deep neural network proposal (\texttt{proposal} in Figure~\ref{fig:proposal-code-figure}), and a set of 10 latent images sampled using importance sampling, with the trained deep neural network proposal as the importance distribution.
The deep neural network was trained on traces and images jointly sampled from the generative model, using ADAM with 170,000 iterations, each with batch size 100.
The deep neural network proposal is uncertain about the location and orientiation of the letter.
Augmenting the neural network with model-based importance sampling gives more accurate inferences.
}
    \label{fig:example-results}
\end{figure}

See Figure~\ref{fig:model-code-figure}.


\section{Related Work}
Guide programs in Pyro,
Using probabilisic programs as proposals,
Edward,
Stuart Russell work on block neural proposals,
Tuan An Le's work on univeral compiled inference,
Wake sleep,
Helmholtz machines,
VAE,
Stochastic inverses

\subsubsection*{Acknowledgments}
This research was supported by DARPA (PPAML program, contract number FA8750-14-2-0004), IARPA (under research contract 2015-15061000003), the Office of Naval Research (under research contract N000141310333), the Army Research Office (under agreement number W911NF-13-1-0212), and gifts from Analog Devices and Google.
This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
